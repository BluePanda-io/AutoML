{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The goal of this project is to create Systematic - Automatic approach of dealing with ML prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we always start with Reading Data, Visualizing Data, Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "from time import gmtime\n",
    "from time import strftime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataFrame = pd.read_csv('Q3-data.csv') \n",
    "print(dataFrame.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before anything starts we need to formulate how to set up our data\n",
    "If this is a supervised learning problem we will need a clear input output data, so we will have to clean this and creat a clear input output connection\n",
    "\n",
    "In this particular problem we can do that using the TV column, we translate this information of TV current cunsamtion into a binarry groundtruth classification between open and closed TV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allBackets = []\n",
    "todayBackets = []\n",
    "housePr = -1\n",
    "for i in range(len(dataFrame.TV)):\n",
    "    \n",
    "    houseNow = int(dataFrame.House[i])-1\n",
    "    if (housePr!=houseNow):\n",
    "        if (housePr!=-1):\n",
    "            allBackets.append(todayBackets)\n",
    "        todayBackets = []\n",
    "        todayBackets.append([dataFrame.TV[i],dataFrame.Agg[i],0])\n",
    "        housePr = houseNow\n",
    "    else:\n",
    "        todayBackets.append([dataFrame.TV[i],dataFrame.Agg[i],0])\n",
    "        \n",
    "        \n",
    "allBackets.append(todayBackets)\n",
    "\n",
    "# print(allBackets[0]) # information of House 0\n",
    "print(allBackets[0][20]) # information of House 0 hour 20\n",
    "print(allBackets[0][20][0]) # information of House 0 hour 20 TV current 0 -> 0 = TV Current / 1 = All House Current / 2 = Ground Truth Open Close\n",
    "\n",
    "\n",
    "\n",
    "MaxMinHouse = np.zeros((2,len(allBackets)))-1 # 0 -> max # 1 -> min\n",
    "MaxMinHouse[1,:] = float(\"Inf\")\n",
    "for houseN in range(len(allBackets)):\n",
    "    for i in range(len(allBackets[houseN])):\n",
    "        if MaxMinHouse[0,houseN]<allBackets[houseN][i][0]:\n",
    "            MaxMinHouse[0,houseN] = allBackets[houseN][i][0]\n",
    "        if MaxMinHouse[1,houseN]>allBackets[houseN][i][0]:\n",
    "            MaxMinHouse[1,houseN] = allBackets[houseN][i][0]\n",
    "            \n",
    "            \n",
    "print(\"max,min = \",MaxMinHouse[0,0],MaxMinHouse[1,0])\n",
    "print(\"max,min = \",MaxMinHouse[0,1],MaxMinHouse[1,1])\n",
    "print(\"max,min = \",MaxMinHouse[0,2],MaxMinHouse[1,2])\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the Data and Changing info to GroundTruth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from functionsModelTrain.cleaningData import balanced_hist_thresholding\n",
    "\n",
    "\n",
    "for houseN in range(len(allBackets)): \n",
    "    showNow = np.zeros((int(MaxMinHouse[0,houseN])+1,1))\n",
    "    printSignal = []\n",
    "    for i in range(len(allBackets[houseN])):\n",
    "        showNow[int(allBackets[houseN][i][0])] = showNow[int(allBackets[houseN][i][0])] + 1\n",
    "        printSignal.append(allBackets[houseN][i][0])\n",
    "\n",
    "    print(\"House = \",houseN)\n",
    "    \n",
    "    print(\"Histogram\")\n",
    "    plt.plot(showNow)\n",
    "    plt.show()\n",
    "\n",
    "    thresh_value = balanced_hist_thresholding(showNow)\n",
    "    print(\"threshold = \",thresh_value)\n",
    "    \n",
    "    \n",
    "    print(\"Raw Signal\")\n",
    "    plt.plot(printSignal)\n",
    "    plt.show()\n",
    "    \n",
    "    showThreshold = []\n",
    "    for i in range(len(allBackets[houseN])):\n",
    "        if (int(allBackets[houseN][i][0])>thresh_value):\n",
    "            allBackets[houseN][i][2] = 1 \n",
    "        else:\n",
    "            allBackets[houseN][i][2] = 0 \n",
    "            \n",
    "        showThreshold.append(allBackets[houseN][i][2])\n",
    "      \n",
    "    print(\"Signal Translated to binary\")\n",
    "    plt.plot(showThreshold)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different visualization using Backets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "maxTV = max(dataFrame.Agg)\n",
    "minTV = min(dataFrame.Agg)\n",
    "print(maxTV,minTV)\n",
    "\n",
    "maxTV = 130\n",
    "\n",
    "backetsAvailable = 50 # This is the numbers of areas that we will have \n",
    "areaPerBacket = (maxTV-minTV)/backetsAvailable\n",
    "\n",
    "todayBackets = np.zeros((backetsAvailable+1,3))\n",
    "for i in range(len(dataFrame.TV)):\n",
    "    if (dataFrame.Agg[i]<130):\n",
    "        backetNow = int(dataFrame.Agg[i]/areaPerBacket)\n",
    "        todayBackets[backetNow,int(dataFrame.House[i])-1] = todayBackets[backetNow,int(dataFrame.House[i])-1] + 1\n",
    "    \n",
    "\n",
    "plt.plot(todayBackets[:,0])\n",
    "plt.show()   \n",
    "plt.plot(todayBackets[:,1])\n",
    "plt.show()   \n",
    "plt.plot(todayBackets[:,2])\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_data (allBackets):\n",
    "    print \"1s in blue, 0s in orange\"\n",
    "    for i in range(3):\n",
    "        print\n",
    "        print \"House \",i\n",
    "        ones = [v for v in allBackets[i] if v[2] == 1]\n",
    "        ones = np.array(ones)\n",
    "        zeros = [v for v in allBackets[i] if v[2] == 0]\n",
    "        zeros = np.array(zeros)\n",
    "\n",
    "        plt.plot(ones[:,1])\n",
    "        plt.plot(zeros[:,1])\n",
    "        plt.show()\n",
    "        print \"ones: \", len(ones)\n",
    "        print \"min\",np.min(ones[:,1]), \"max\", np.max(ones[:,1]), \"mean\", np.mean(ones[:,1])\n",
    "        print\n",
    "        print \"zeros: \",len(zeros)\n",
    "        print \"min\",np.min(zeros[:,1]), \"max\", np.max(zeros[:,1]), \"mean\", np.mean(zeros[:,1])\n",
    "        print \"------------------------------------------\"\n",
    "        \n",
    "visualise_data(allBackets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functionsModelTrain.cleaningData import find_outliers\n",
    "\n",
    "house0 = allBackets[0]\n",
    "house1 = allBackets[1]\n",
    "house2 = allBackets[2]\n",
    "\n",
    "data = []\n",
    "data.extend(house0) \n",
    "data.extend(house1) \n",
    "data.extend(house2)\n",
    "\n",
    "data = np.array(data)\n",
    "data = data[:,1] \n",
    "print len(house0), len(house1), len(house2)\n",
    "print len(data)\n",
    "\n",
    "lower_bound, upper_bound = find_outliers (data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNDERSAMPLING (REMOVE OUTLIERS FROM DATA) & VISUALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersampling (allBackets):\n",
    "\n",
    "    underSampl1 = [v for v in allBackets[0] if v[1] < upper_bound]\n",
    "    underSampl2 = [v for v in allBackets[1] if v[1] < upper_bound]\n",
    "    underSampl3 = [v for v in allBackets[2] if v[1] < upper_bound]\n",
    "\n",
    "    return underSampl1,underSampl2,underSampl3\n",
    "\n",
    "\n",
    "underSampl1,underSampl2,underSampl3 = undersampling(allBackets)\n",
    "\n",
    "allBackets2=[]\n",
    "allBackets2.append(underSampl1)\n",
    "print (len(allBackets2[0]))\n",
    "allBackets2.append(underSampl2)\n",
    "print (len(allBackets2[1]))\n",
    "allBackets2.append(underSampl3)\n",
    "print (len(allBackets2[2]))\n",
    "\n",
    "print (allBackets2[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_data(allBackets2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate into Train and Test Data with multiple different combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functionsModelTrain.trainTest import train_test_sets\n",
    "\n",
    "\n",
    "#ALL HOUSES ORIGINAL\n",
    "train_data_org , train_labels_org, test_data_org , test_labels_org = train_test_sets(allBackets, 0.7, House0=1, Outliers_train=1, Outliers_test=1)\n",
    "#ALL HOUSES :TRAIN_DATA -> NO OUTLIERS\n",
    "train_data , train_labels,test_data , test_labels = train_test_sets(allBackets, 0.7, House0=1, Outliers_train=0, Outliers_test=1)\n",
    "#ALL HOUSES : TRAIN_DATA & TEST DATA -> NO OUTLIERS\"\n",
    "train_data_noOUT , train_labels_noOUT , test_data_noOUT  , test_labels_noOUT = train_test_sets(allBackets, 0.7, House0=1, Outliers_train=0, Outliers_test=0)\n",
    "\n",
    "\n",
    "#ORIGINAL NO HOUSE 0\n",
    "train_data_noHouse0_org , train_labels_noHouse0_org, test_data_noHouse0_org  , test_labels_noHouse0_org = train_test_sets(allBackets, 0.7, House0=0, Outliers_train=1, Outliers_test=1)\n",
    "#NO HOUSE 0 : TRAIN_DATA -> NO OUTLIERS\n",
    "train_data_noHouse0 , train_labels_noHouse0 ,test_data_noHouse0  , test_labels_noHouse0= train_test_sets(allBackets, 0.7, House0=0, Outliers_train=0, Outliers_test=1)\n",
    "#\"NO HOUSE 0 : TRAIN_DATA & TEST DATA -> NO OUTLIERS\"\n",
    "train_data_noHouse0_noOUT , train_labels_noHouse0_noOUT , test_data_noHouse0_noOUT  , test_labels_noHouse0_noOUT = train_test_sets(allBackets, 0.7, House0=0, Outliers_train=0, Outliers_test=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test different models in your trainin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functionsModelTrain.models import logisticRegression_classifier\n",
    "\n",
    "print \"ALL HOUSES ORIGINAL\"\n",
    "modelLogReg = logisticRegression_classifier(train_data_org, train_labels_org)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST model and accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functionsModelTrain.testModelAccur import accuracyScoreModel,confussionMatr,classification_reportRes\n",
    "\n",
    "print(accuracyScoreModel(modelLogReg,test_data,test_labels))\n",
    "print(confussionMatr(modelLogReg,test_data,test_labels))\n",
    "print(classification_reportRes(modelLogReg,test_data,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functionsModelTrain.models import RandomForest_Classifier\n",
    "\n",
    "\n",
    "modelRFC = RandomForest_Classifier(train_data_org, train_labels_org,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functionsModelTrain.testModelAccur import accuracyScoreModel,confussionMatr,classification_reportRes\n",
    "\n",
    "print(accuracyScoreModel(modelRFC,test_data,test_labels))\n",
    "print(confussionMatr(modelRFC,test_data,test_labels))\n",
    "print(classification_reportRes(modelRFC,test_data,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functionsModelTrain.models import SVM_classifier\n",
    "\n",
    "\n",
    "modelSVM = SVM_classifier(train_data_org, train_labels_org,'rbf',30,'auto')\n",
    "# modelSVM = SVM_classifier(train_data_org, train_labels_org)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functionsModelTrain.testModelAccur import accuracyScoreModel,confussionMatr,classification_reportRes\n",
    "\n",
    "print(accuracyScoreModel(modelSVM,test_data,test_labels))\n",
    "print(confussionMatr(modelSVM,test_data,test_labels))\n",
    "print(classification_reportRes(modelSVM,test_data,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functionsModelTrain.models import NeuralNetwork_Classifier\n",
    "\n",
    "\n",
    "# modelNN = NeuralNetwork_Classifier(train_data_org, train_labels_org,[5,12,18,12,5],500)\n",
    "modelNN = NeuralNetwork_Classifier(train_data_org, train_labels_org,[11,18,11],1500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functionsModelTrain.testModelAccur import accuracyScoreModel,confussionMatr,classification_reportRes\n",
    "\n",
    "print(accuracyScoreModel(modelNN,test_data,test_labels))\n",
    "print(confussionMatr(modelNN,test_data,test_labels))\n",
    "print(classification_reportRes(modelNN,test_data,test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch for HyperParameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from functionsModelTrain.hyperParameterSearch import hyperParameterSearch\n",
    "\n",
    "# hyperParameters = {\n",
    "#     'hidden_layer_sizes':[[10,10],[5,10,5]],\n",
    "#     'max_iter':[500,1000,2000]\n",
    "# }\n",
    "hyperParameters = {\n",
    "    'hidden_layer_sizes':[[10,10],[5,10,5],[5,10,20,10,5],[5,5,5]],\n",
    "    'max_iter':[500,1000,2000,5000]\n",
    "}\n",
    "\n",
    "train_data_orgNow = train_data_org.reshape(-1,1)\n",
    "\n",
    "mlpc = hyperParameterSearch(MLPClassifier(),hyperParameters,\"randG\",10,train_data_orgNow, train_labels_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(mlpc.cv_results_)\n",
    "print(df[['param_hidden_layer_sizes','param_max_iter','mean_test_score']])\n",
    "\n",
    "print(mlpc.best_score_)\n",
    "print(mlpc.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functionsModelTrain.models import NeuralNetwork_Classifier\n",
    "\n",
    "\n",
    "hidden_layerN = mlpc.best_params_[\"hidden_layer_sizes\"]\n",
    "max_iterN = mlpc.best_params_[\"max_iter\"]\n",
    "modelNN = NeuralNetwork_Classifier(train_data_org, train_labels_org,hidden_layerN,max_iterN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functionsModelTrain.testModelAccur import accuracyScoreModel,confussionMatr,classification_reportRes\n",
    "\n",
    "print(accuracyScoreModel(modelNN,test_data,test_labels))\n",
    "print(confussionMatr(modelNN,test_data,test_labels))\n",
    "print(classification_reportRes(modelNN,test_data,test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Classifiers\n",
    "\n",
    "Here we will calcualte accuracy score, but we always have to compare this with nul acuracy which is what is the persentage if we just calucate everything to be equal to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "test_data = test_data.reshape(-1,1)\n",
    "\n",
    "predictions = modelNN.predict(test_data)\n",
    "\n",
    "print(metrics.accuracy_score(test_labels,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1 - test_labels.mean()) # This is the minimum prediction if the system is completly random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking the actual predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepN = 15\n",
    "numberLines = 3\n",
    "\n",
    "startPoint = 0\n",
    "\n",
    "\n",
    "for i in range(len(test_labels)/stepN):\n",
    "    print(i)\n",
    "    print(\"True: \",test_labels[i:i+stepN])\n",
    "    print(\"Pred: \",predictions[i:i+stepN])\n",
    "    \n",
    "    ConfMatrix = []\n",
    "    for kt in range(stepN):\n",
    "        if (test_labels[i+kt]==1 and predictions[i+kt]==1):\n",
    "            ConfMatrix.append(\"TP\")\n",
    "        elif (test_labels[i+kt]==0 and predictions[i+kt]==0):\n",
    "            ConfMatrix.append(\"TN\")\n",
    "        elif (test_labels[i+kt]==1 and predictions[i+kt]==0):\n",
    "            ConfMatrix.append(\"FN\")\n",
    "        elif (test_labels[i+kt]==0 and predictions[i+kt]==1):\n",
    "            ConfMatrix.append(\"FP\")\n",
    "            \n",
    "    print(\"ConM:'array\",ConfMatrix)\n",
    "            \n",
    "    if (i>=numberLines):\n",
    "        break\n",
    "    \n",
    "    \n",
    "    startPoint = startPoint + stepN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functionsModelTrain.testModelAccur import VisualizePredTestResults\n",
    "\n",
    "\n",
    "VisualizePredTestResults(modelNN,test_data,test_labels,12,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
